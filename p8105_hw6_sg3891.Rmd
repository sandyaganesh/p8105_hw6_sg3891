---
title: "p8105_hw6_sg3891"
author: Sandya Ganesh
date: 2021-12-04
output: github_document
---

```{r setup, echo = FALSE, message = FALSE}
library(tidyverse)
library(dplyr)
library(modelr)
library(mgcv)

knitr::opts_chunk$set(
  fig.height = 6,
  fig.width = 8,
  message = F,
  warning = F 
  )

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

set.seed(1)
```
## Question 1

#### Load and clean the birthweight data for regression analyses. 
```{r}
birthweight = 
  read_csv("./data/birthweight.csv")

skimr::skim(birthweight)
```
Looking into this data set, we can see that the total number of observations  is `r nrow(birthweight)` observations with `r ncol(birthweight)` variables. The dataset includes variables such as `r names(birthweight %>% select(c(1,2,3,4,5,6)))`. 
Additionally, all of the variables are numeric and there are no missing values.

Next, we will convert numeric to factor variables.
```{r}
birthweight = birthweight %>% 
  mutate(
    babysex = factor(babysex),
    babysex = fct_recode(babysex, "male" = '1', "female" = '2'),
    frace = factor(frace),
    frace = fct_recode(frace,
                       "White" = '1',
                       "Black" = '2',
                       "Asian" = '3',
                       "Puerto Rican" = '4',
                       "Other" = '8',
                       "Unknown" = '9'),
    malform = factor(malform),
    malform = fct_recode(malform,
                         "absent" = '0',
                         "present" = '1'),
    mrace = factor(mrace),
    mrace = fct_recode(mrace,
                       "White" = '1',
                       "Black" = '2',
                       "Asian" = '3',
                       "Puerto Rican" = '4',
                       "Other" = '8')
     )
```

Model 1: 
I will build my model using backwards selection, and validating the predictors we are including using the current literature and my hypotheses. For the backwards selection, we will start with a fully saturated model, and then run backwards selection to remove all non-significant predictors.

```{r}
model1 = lm(bwt ~ ., data = birthweight) %>% 
  MASS::stepAIC(direction = "backward", trace = FALSE)

model1 %>% broom::tidy() %>% knitr::kable(digits = 3)
```
Considering an alpha of 0.05 for significance, our model now contains  statistically significant predictors of baby's birthweight, except for fincome (family's monthly income). To validate whether this predictor should be included though it is not statistically significant, I looked into the literature, and found that family income is positively associated with baby's birthweight, with lower incomes predicting lower birthweights. This makes sense, as income can affect nutrition of the mother during pregnancy and other social determinants of health.

I also looked at all of the significant predictors to ensure they are variables  used in the literature when looking at birth outcomes. Factors relating to the mother such as maternal race and height, as well as factors relating to the baby such as sex and length often appear in the literature, so I feel comfortable including them. Additionally, the literature shows that smoking during pregnancy can cause a lot of malformations in children and can affect birthweight as well, so it makes sense to include number of cigarettes smoked as a predictor.

Plot of model residuals against fitted values
```{r}
model1 = lm(bwt ~ menarche + fincome + smoken, data = birthweight)

model1 %>% broom::tidy() %>% knitr::kable(digits = 3)

add_residuals(birthweight, model1) %>% add_predictions(model1) %>% 
  ggplot(aes(x = resid, y = pred)) + geom_point(alpha = 0.5) +
  labs(x = "Predicted Birthweight (g)",
       y = "Residuals",
       title = "Relationship between Predicted Birthweight and Residuals")
```

#### Additional Models and Cross Validation

Model with length at birth and gestational age as predictors
```{r}
model2 = lm(bwt ~ 
              blength + gaweeks, 
              data = birthweight)

```

Model with head circumference, length, sex, and all interactions as predictors
```{r}
model3 = lm(bwt ~ 
              bhead + blength + babysex + 
              bhead * blength + bhead * babysex + blength * babysex + 
              bhead * blength * babysex, 
              data = birthweight)
```

In order to do the cross validation, we will first create the crossv_mc tibble with the train and test data
```{r}
cv_df = crossv_mc(birthweight, 100) %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
  )
```

Next, we will fit the models and get the root mean squared errors
```{r}
fitted_df = cv_df %>%
  mutate(
    model1 = map(train, 
                 ~lm(bwt ~ babysex + bhead + blength + delwt + fincome + gaweeks + mheight + mrace + parity + ppwt + smoken, 
                     data =.x)),
    model2 = map(train,
                  ~lm(bwt ~ blength + gaweeks, 
                     data = .x)),
    model3 = map(train,
                  ~lm(bwt ~ 
                      bhead + blength + babysex + 
                      bhead * blength + bhead * babysex +
                      blength * babysex +
                      bhead * blength * babysex,
                      data = .x))) %>% 
  mutate(
    rmse_model1 = map2_dbl(model1, test, ~rmse(model = .x, data = .y)),
    rmse_model2  = map2_dbl(model2, test, ~rmse(model = .x, data = .y)),
    rmse_model3  = map2_dbl(model3, test, ~rmse(model = .x, data = .y))
)
```

Finally, we will plot the distribution of RMSE values to compare each candidate model
```{r rsme_plot}
fitted_df %>%
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + geom_violin(alpha = 0.5) +
  labs(
    x = "Regression Model",
    y = "RSME",
    title = "Birthweight Regression Models and RSME values"
  )
  
```


From the plot above, we can see that the RSMEs are on average lowest for model 1, then model 3, and finally model 2 has the highest RSME values. This shows that model 1, the model that I have created using the backwards selection and literature searches, is the best choice in terms of prediction error, as it minimizes the average distance between the actual birth weight and the predicted birthweight. The model that I have selected includes no interaction terms, as opposed to model 3. The predictors in my final model (model 1) are sex of the baby, head circumference, baby length, delivery weight, family income, gestational age, mother's height, mother's race, parity, pre-pregnancy weight, and number of cigarettes smoked.

## Question 2

First, we will download the 2017 Central Park weather data
```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

We will draw 5000 bootstrap samples and produce estimates of the quantiles.
We will work on a simple linear regression model, with tmax as the outcome and tmin as the predictor.

#### R squared coefficient of determination
```{r rsquared}
boot_straps_r_weather =
  weather_df %>% 
  modelr::bootstrap(n = 5000) %>% 
  mutate(
    models = map(strap, ~lm( tmax ~ tmin, data = .x) ),
    results = map(models, broom::glance)) %>%
  select(-strap, -models) %>% 
  unnest(results)
  
boot_straps_r_weather %>% 
  janitor::clean_names() %>% 
  summarize(
    lower_limit = quantile(r_squared, c(.025)),
    upper_limit = quantile(r_squared, c(.975))) %>% 
  knitr::kable()

boot_straps_r_weather %>% 
  ggplot(aes(x = r.squared)) + 
  geom_density() +
  labs(
    x = "R Squared Values",
    y = "Density",
    title = "Distribution of R-Squared estimates for Central Park Weather"
  ) 
  
```
We can see that the distribution of the R-squared data is relatively normally distributed.
The 95% CI for the r-squared values is (0.894, 0.927), as seen in the table above.

#### Log(B0*B1) 
```{r logb0b1}
boot_straps_log_weather =
  weather_df %>% 
  modelr::bootstrap(n = 5000) %>% 
  mutate(
    models = map(strap, ~lm( tmax ~ tmin, data = .x) ),
    results = map(models, broom::tidy)) %>%
  select(-strap, -models) %>% 
  unnest(results) %>% 
  janitor::clean_names() %>%
  select(id, term, estimate) %>% 
  pivot_wider(
    names_from = "term",
    values_from = "estimate"
  ) %>% 
  rename(intercept = '(Intercept)') %>% 
  mutate(log_b0b1 = log(intercept*tmin))

boot_straps_log_weather %>% 
  summarize(
    lower_limit = quantile(log_b0b1, c(.025)),
    upper_limit = quantile(log_b0b1, c(.975))) %>% 
  knitr::kable()

boot_straps_log_weather %>% 
  ggplot(aes(x = log_b0b1)) + 
  geom_density() +
  labs(
    x = "Log(B0*B1)",
    y = "Density",
    title = "Distribution of Log(B0*B1) estimates for Central Park Weather"
  ) 
```

We can see that the distribution of the log(B0*B1) data is relatively normally distributed.
The 95% CI for the r-squared values is (1.966, 2.058), as seen in the table above.

